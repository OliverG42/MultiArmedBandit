\chapter{Introduction}
\label{cha:chapter1} % (labels for cross referencing)


%% 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}

%%
\newcommand{\armDistribution}[1]{\mathrm{P}_{#1}}
\newcommand{\armPopulationMean}[1]{\mu_{#1}}
\newcommand{\armDistributionVect}{\mathrm{P}}

\newcommand{\action}[1]{A_{#1}}

\newcommand{\reward}[2]{
   \ifthenelse{\isempty{#2}}{
   Y_{#1}(\action{#1})}{
   Y_{#1}(#2)}}

\newcommand{\policy}{\varphi}

\newcommand{\cumulativeRegret}[2]{\mathcal{R}_{#1}(#2)}



The Multi-Armed Bandit problem (often abbreviated to MAB), is a framework in machine learning and decision theory, in which, an agent is presented with a set of actions, each with an unknown reward distribution assigned to it. The agent's objective is to attempt to maximise it's cumulative reward over a period of time by analysing the information it gathers from performing actions.

For the purpose of this paper, we define the following:

We will consider a multi-armed bandit problem with $K$ arms and a time horizon $T$, where $K$, $N \in \N$.

A MAB with $K$ arms with distributions $\armDistributionVect = (\armDistribution{1}, \dots , \armDistribution{K})$, such that arm $i$ has distribution $\armDistribution{k}$. Let $\armPopulationMean{1}, \dots , \armPopulationMean{K}$ be the mean of the above distributions.


For each time step $t \in [T]:=\{1,\ldots,T\}$, an arm $\action{t}$ is chosen, and a corresponding reward is observed as $\reward{t}{}$, where the random rewards $\reward{t}{k} \sim  \armDistribution{k}$ are drawn independently. We shall assume that our actions  $\action{t}$ are selected in accordance with some policy $\policy$. Here a policy $\policy: \bigcup_{\ell \in \N}([K]\times \R)^\ell \times [0,1] \rightarrow [K]$ denotes a function for selecting actions, so that at each time step $t \in [T]$, we have $\action{t}= \policy((\action{1},\reward{1}{}),\ldots,(\action{t-1},\reward{t-1}{}),W_t)$, with $W_t$ denoting an independent random variable.


The (random) cumulative regret for a policy $\policy$ is then defined by
\begin{align*}
\cumulativeRegret{T}{\policy}:=T \cdot \max_{k \in [K]} \armPopulationMean{k}-\sum_{t \in [T]}\armPopulationMean{\action{t}},
\end{align*}
where the actions $\action{t}$ are selected via the policy $\policy$.


\section{The Bernoulli Bandit Vs The Gaussian Bandit}
\label{sec:BernoulliBandit}

The Bernoulli bandit problem and the Gaussian bandit problem are both sequential decision-making problems:

In the Bernoulli bandit, the rewards are drawn from a Bernoulli distribution of unknown mean, which manifests as a binary result between two possible outcomes (normally success or failure). The goal is to select arms in a strategic manner to maximize the cumulative reward. This should balance exploration to learn more about other arm's expected means, and exploitation of the best-performing arms.

In contrast, the Gaussian bandit involves rewards drawn from a Gaussian distribution with unknown variance and mean, which manifests as a continuous outcome between [0, 1]. The objective is identical to the Bernoulli bandit, but the focus is on estimating unknown means and using information to make optimal decisions.

\subsection{Algorithmic Approaches}
\label{sec:Algorithms}

\subsubsection{Randomized}
\label{sec:randomized}
The randomized algorithm for multi-armed bandits involves selecting arms at random with equal probability, without considering any feedback or past performance. This algorithm is commonly referred to as the "purely random" or "uniform random" algorithm.

The random algorithm can be summarized as follows:

\pseudobox{%
  \KwIn{List of arms $\mathcal{A}$ with unknown probabilities $\mathcal{P}$}
  \KwOut{Selection strategy}

  \BlankLine
  \For{$t = 1,2,\ldots$}{
    Choose a random arm $A_t = a$ uniformly from the set of arms $1,2,\ldots,K$;\newline
    Receive the reward $R_t$ by pulling arm $a$;
  }
}{Randomized Algorithm}

At each time step $t$, the randomized algorithm randomly selects an arm $A_t$ from $\mathcal{A}$, following a uniform distribution:

$$P(A_t = a) = \frac{1}{{|\mathcal{A}|}} \quad \text{for all } a \in \mathcal{A}$$

This algorithm is quite often used as a baseline for more sophisticated algorithm evaluation. In the long run, the expected rewards, $R_t$, converge towards the average of the probabilities of each arm:

$$lim_{{t \to \infty}} R_t = \frac{1}{|\mathcal{A}|} \sum_{{a \in \mathcal{A}}} P(A_t = a)$$

Although this algorithm is very simple to implement, it doesn't attempt to optimize arm selection based on past events. It can be described as a purely exploratory algorithm that doesn't exhibit any exploitative behavior, hence its performance is generally vastly inferior to that of any other algorithms.

\subsubsection{Greedy}
\label{sec:Greedy}
The greedy algorithm is a step-up from randomly picking arms, since it actually takes into account past performance. Put simply, it chooses the arm that has the highest estimated expected reward, denoted by the arm with the highest current rate of success.

To begin, it assigns each arm with a "action-value estimate", essentially the predicted success rate of each arm. This is often set arbitrarily at some value, usually 0.5 or 1.

Then, at each time step $t$, the greedy algorithm selects the arm $A_t$ from $\mathcal{A}$ with the highest expected success rate.

The greedy algorithm can be summarized as follows:

\pseudobox{%
    \KwIn{List of arms $\mathcal{A}$ with unknown probabilities $\mathcal{P}$, action-value estimate $\mathcal{E}$}
    \KwOut{Selection strategy}
    \BlankLine
    \ForEach{arm $i = 1$ \KwTo $K$}{
        Initialise success rate $Q_i \leftarrow \mathcal{E}$\newline
        Initialise number of successes $S_i \leftarrow \mathcal{0}$\newline
        Initialise count $C_i \leftarrow 0$
    }
    \BlankLine
    \For{$t = 1,2,\ldots$}{
        Choose arm $A_t = a$ with the highest success rate: $a \leftarrow \arg\max_i Q_i$\newline
        Receive the reward $R_t$ by pulling arm $a$;\newline
        Increment count: $N_a \leftarrow N_a + 1$\newline
        Increment successes: $S_a \leftarrow S_a + R_t$\newline
        Update success rate: $Q_a \leftarrow \frac{S_a}{N_a}$

  }
}{Greedy Algorithm}

Although this algorithm is simple and fast, it tends to exploit the immediate best option, and very rarely explores other options to try and optimise it's rewards.

It also is extremely sensitive to initial results, which significantly impacts it's performance. For example, if a 3-arm bandit has arms (0.4, 0.75, 0.8), it could get "unlucky" with reasonable probability 2\% to get rewards (1, 0, 0), which effectively locks it in to keep pulling arm 0, even though the other two arms are much better.

Moreover, the value of E can significantly impact the algorithm's performance - in the extreme case where E=0, the algorithm will keep pulling the first arm that succeeds, and will only explore other arms when the programming language registers the success rate as so low, it rounds down to 0. For example, Python will do this only when the success rate goes below $10^{-324}\%$

\subsubsection{Epsilon Greedy}
\label{sec:EpsilonGreedy}
Describe high-level intuition
Pseudo-code
Advantages and Disadvantages.

\subsubsection{Upper Confidence Bound (UCB)}
\label{sec:UCB}
Describe high-level intuition
Pseudo-code
Advantages and Disadvantages.

\subsubsection{Thompson}
\label{sec:Thompson}
Describe high-level intuition
Pseudo-code
Advantages and Disadvantages.