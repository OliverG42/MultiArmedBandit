\chapter{Pure Exploration}
\label{cha:pureexploration}

\section{The Premise}
\label{sec:premise}
Moving on from our discussion of exploration-exploitation algorithms in MAB problems, I'll now explore the concept of pure exploration. Unlike exploration-exploitation, which has to balance between exploiting the best arm whilst exploring for better ones, pure exploration simplifies this by focusing solely on exploration. This means it doesn't care about being penalized choosing sub-optimal arms, as it's goal is to find the best arm as fast as possible with some certainty value.

The key difference lies in deciding when to stop, known as the $\textbf{stopping time}$ $\stoppingTime$. In traditional bandit scenarios, this stopping time sets the maximum number of arm selections allowed. While there's typically an upper limit on $\stoppingTime$, Pure Exploration agents tend to reach a stopping time below this threshold.

\section{Definition of Pure Exploration Agents}

\begin{definition}\label{def:method}
The triple $(\policy, \stoppingTime, \selectionRule)$ is a \textbf{method} $\method$, given a policy $\policy$, stopping time $\stoppingTime$, and selection rule $\selectionRule$.

\begin{enumerate}
    \item \textbf{Policy} $\policy$: A function that returns which arm to select at each timestep
    
    \item \textbf{Stopping Time} $\stoppingTime$: A function that determines if the agent stops exploring
    
    \item \textbf{Selection Rule} $\selectionRule$ : A function that identifies the optimal arm upon cessation of exploration

    Usually, this is defined intuitively as the arm with the highest empirical mean, that is: $ \arg\max_{a}\left[\empiricalMeanReward{a}{\stoppingTime}\right]$
    
\end{enumerate}
\end{definition}

\section{Aims of Pure Exploration Agents}
\label{sec:simpleregret}

Since in pure exploration we no longer care about cumulative regret, we have to redefine our performance measure. One popular method is using simple regret, which is defined as "the expected regret to be chosen after time t by policy $\policy$":

\begin{align}
\simpleRegret{t, \policy}{\policy} = \Ex_{\policy}(\gap{A_{t+1}}).
\end{align}

Intuitively, relating to our Ice Cream example \ref{ex:ice-cream}, this represents the expected performance difference between the best ice cream and the selected one. Naturally, we want this to be as small as possible, to minimize the probability we choose a worse ice cream, although we are constrained by two factors:

\begin{itemize}
    \item How long we have (t)
    \item How certain we want to be we've got the best arm ($\rho$)
\end{itemize}

\section{Simple Algorithms for Pure Exploration}

Uniform exploration is the simplest form of algorithm for pure exploration scenarios. Similarly to the Greedy algorithm \ref{sec:Greedy}, it samples arms in such a way that the outcome is a uniform selection. However for the Uniform exploration policy, we ensure each arm is selected perfectly uniformly:

\pseudobox{%
    \KwIn{List of arms $\armsList$ with unknown distributions $\armDistributionVect$}
    \KwOut{Selection strategy, best arm}
    \BlankLine
    \ForEach{arm $i = 1$ \KwTo $K$}{
        Initialise number of successes $S_i \leftarrow 0$
    }
    \BlankLine
    \For{$t = 1,2,\ldots$}{
        Choose arm $A_t = t \mod{K}$\newline
        Receive the reward $R_t$ by pulling arm $a$;\newline
        Increment successes: $S_a \leftarrow S_a + R_t$\newline
     }
    return arm $A_i$ with the highest number of successes: $A_i \leftarrow \arg\max_i S_a$ \newline

}{Uniform Algorithm}

\section{Fixed Confidence}
\label{sec:fixedconfidence}

In the previous chapter, we had some notion of certainty of our decisions, such as UCB \ref{sec:UCB} and Ripple \ref{sec:ripple}. However, our understanding was obfuscated by the exploration and exploitation factors. An exception to this is when $t \rightarrow \infty$, but this is unrealistic, and doesn't align with real-world situations, as demonstrated by the Substance Synthesis example \ref{ex:substance-synthesis}.

However, in pure exploration, our only "drive" is exploration, which makes defining a failure probability parameter $\failureProb$ much easier. Hence, an algorithm's goal is to find the best arm, with $(1-\failureProb)$ certainty, with as few samples as possible i.e before some stopping time $\stoppingTime$. Naturally, the exact value of $\failureProb$ will vary depending on the scenario - for our Substance Synthesis example \ref{ex:substance-synthesis}, we may have our $\failureProb$ relatively high, if we expect there to be a very large difference in the results of each resistor. However, in the later stages, when changes are comparatively much smaller, we may choose a very small value for $\failureProb$ so that we're very certain which changes have made an improvement.

In order to compare any strategies, let us define:

\begin{definition}\label{def:soundess}
A method  $\method = (\policy, \stoppingTime, \selectionRule)$ is said to be \textbf{sound} with failure probability $\failureProb$ if for all  $v \in \banditSpace$,
\begin{align}
\Prob_{}(\stoppingTime < \infty  \text{ and }  \gap{\selectionRule}(v) > 0) \leq \failureProb.
\end{align}

Equivalently we have
\begin{align}
\Prob_{}(\stoppingTime < \infty  \text{ and }  \gap{\selectionRule}(v) = 0) \geq 1-\failureProb.
\end{align}

\end{definition}

In other words, a triple is sound if, which probability of failure $\failureProb$: The policy stops, and the current gap is optimal. The $\stoppingTime < \infty$ is needed, since a triple that doesn't stop is meaningless.

Naturally, we desire some method that has policy $\policy$ that minimizes the stopping time $\stoppingTime$ and failure probability $\failureProb$. However, similar to exploration-exploitation bandits, we must strike some balance between efficiency and certainty - increasing the stopping time of $\stoppingTime$ decreases the confidence $\failureProb$ that the chosen arm is optimal. Conversely, letting the confidence $\failureProb$ increase slightly can decrease the stopping time from $\stoppingTime$ a large amount.


\section{Track-and-Stop Strategies}
\label{sec:trackandstop}
% Explain Section 33.2.2 and try to make sense of Algorithm 21 and Theorem 33.6.
It has been shown previously that the expected stopping time $\stoppingTime$ for a MAB with fixed confidence $\failureProb$ is bounded by below as follows:

\begin{theorem}\label{thm:fixed-confidence}
For a MAB with arm distributions $\armDistributionVect$, some method $\method = (\policy, \stoppingTime, \selectionRule)$, and failure probability $\failureProb$, $\forall \bandit \in \banditSpace:$

%Theorem 33.5 in Lattimore

\begin{align}
\Ex(\stoppingTime) \geq c^*(\bandit) \times log\left({\frac{1}{4(1-\failureProb)}}\right)
\end{align}

\begin{align}
\text{for } c^*(\bandit)^{-1} = \sup_{\alpha \in \mathcal{P}_{k-1}}\left(
\inf_{\bandit^{'} \in \banditSpace_{alt}(\bandit)}\left(
\sum_{i=1}^{k} \alpha_{i} \relativeEntropy{\bandit_{i}}{\bandit_{i}^{'}}
\right)
\right)
\end{align}
\end{theorem}

We will focus on the \textbf{Gaussian} setting for this section. Hence, we can define the \textbf{relative entropy} \relativeEntropy{u}{v} of two Gaussian distributions. Assuming they both have identical variances $\sigma_i$, this is:

\begin{align}
\relativeEntropy{\bandit}{\bandit^{'}} \coloneqq \frac{\left( \armPopulationMeanSpecific{i}{\bandit} - \armPopulationMeanSpecific{i}{\bandit^{'}}\right)^2}{2\sigma_i^2}
\end{align}

We're also using a \textbf{probability simplex} $\mathcal{P}_{k-1}$, which is a k-dimensional unit vector, in that $\mathcal{P}_{k-1} = \{ \mathbf{x} \in \left[0, 1 \right]^{k}, \|\mathbf{x}\| = 1
 \}$. (For example, we could have $\alpha = [0.2, 0.35, 0.45] \in \mathcal{P}_{2}$)

$c^*(\bandit)^{-1}$ appears a bit strange, however what it's doing is it's essentially performing a maximin game with itself. Essentially, $c^*(\bandit)^{-1}$ represents the maximum value achievable for the smallest potential difference between two diverse-prediction bandits, considering a simplex $\alpha$.

Sadly, we cannot construct an algorithm that outperforms this \ref{thm:fixed-confidence} lower bound; however, we can create one that ensures all potential outcomes are as close to the lower bound as possible. In order to do this, such an algorithm would have to estimate each arm's underlying mean $\armPopulationMean{a}$ by sampling in proportion to the empirical mean $\empiricalMeanReward{a}{t}$ for $t \leq \stoppingTime$.

However, this can lead to some arms being "starved" if we initially get unfortunately poor results from the first few samplings, similarly to greedy strategies mentioned previously\ref{sec:Greedy}. Hence, we must impose some "resource equity" by requiring us to sample each arm at least some number of times at each time measure, which hopefully ensures $\empiricalMeanReward{a}{t}$ is close to $\armPopulationMean{a}$ for sufficiently large $t$.


Part of what makes pure exploration algorithms interesting is that they can terminate early whilst not having a narrow uncertainty around each arm's true mean $\armPopulationMean{a}$. This is because they typically hold estimates of each arm's true distribution $\armDistribution{a}$, which is updated at each time step. The uncertainty can then be calculated with respect to these estimated distributions. 

The high-level intuition of the Track-and-Stop algorithm is as follows:

\begin{itemize}
    \item While there is a reasonable probability multiple arms could be optimal:
    \begin{itemize}
        \item If the arm that has been selected least often hasn't been selected at least $\sqrt{t}$ times:
        \begin{itemize}
            \item Choose this arm
        \end{itemize}
        \item Otherwise:
        \begin{itemize}
            \item Choose arm i that maximizes (t * average) - no. times selected
        \end{itemize}
    \end{itemize}
    \item Set the selection rule to the arm with the highest mean at this time-step, and set the stopping time
\end{itemize}

The pseudo-code for the Track-and-Stop algorithm is defined as follows:

\pseudobox{%
    \KwIn{Confidence $\failureProb$, $\beta_t(\failureProb)$}
    \KwOut{Selection strategy}
    \BlankLine
    t = 0\newline
    \ForEach{arm $i = 1$ \KwTo $K$}{
        Choose arm $A_{t+1} \leftarrow i$
    }
    
    \BlankLine
    \Fn{\policyTAS{$t$}}{
        \If{$\arg\min_{i \in [K]}\totalFunction{i}{t} \leq \sqrt{t}$}{
            Return arm $j \leftarrow \arg\min_{i \in [K]} \totalFunction{i}{t}$;
        }
        \Else{
            Return arm $j \leftarrow \arg\max_{i \in [K]} (t * \empiricalMeanReward{a}{t} - \totalFunction{i}{t})$;
        }
    }
    \BlankLine
    
    \While{$Z_t < \beta_t(\failureProb)$}{
        Choose arm $A_{t+1} \leftarrow \policyTAS{t}$;
     }
    return $i^*(\hat{v}(t))$, stopping time $\stoppingTime = t$\newline

}{Track-And-Stop Algorithm}

We define $Z_t := \defineVagueZedTee{t}$ and $\beta_t(\failureProb)$ to be some function that determines stopping time.

In the case where each bandit arm is modelled as a Gaussian distribution with identical variances, $Z_t = \defineZedTee{t}$

%\[\frac{1}{2}\inf\limits_{\aDifBandit \in \banditSpace_{alt}(\vectorBanditMeans(X))}
%\sum_{i=1}^k \totalFunction{i}{X} \left( \vectorBanditMeans_i(X) - \aDifBandit_i\right)^2\]

We now have to show the Track-And-Stop algorithm is sound\ref{def:soundess} with $\failureProb$, given some stopping time $\stoppingTime = \beta_t(\failureProb)$, which we do as follows:

We need to check the policy $\policy=\policyTAS$, stopping time $\stoppingTime(\failureProb)=Z_t < \beta_t(\failureProb)$ and selection rule $\selectionRule(t) = i^*(\hat{v}(t))$ are well defined, otherwise the algorithm is invalid. $\policyTAS$ always returns some singular arm selection (since we can break ties arbitrarily). Skipping ahead, $\selectionRule(t)$ can also only return some singular arm selection, as if we have a tie for the empirically optimal arm (that is $|\maxMeanArm(\stoppingTime)| \neq 1$), note $Z_t=0$, hence we only stop and use our selection rule if exactly one arm has the highest empirical mean.

Our stopping time is more complicated, since we must first define $\beta_t(\cdot)$ to check if $\stoppingTime(\cdot)$ is well defined. Now, we note that:
\begin{align*}
&\mathrel{\phantom{=}}\left\{\textbf{bandits which disagree with the empirical distribution over what the best arm is} \right\} \\
&\mathrel{\phantom{=}}\subseteq \left\{\textbf{bandits where the algorithm stops at our stopping time}\right\}
\end{align*}

Which leads to:
\begin{align}
\left\{ v \in \banditSpace_{alt}(\maxMeanArm({\stoppingTime})) \right\} \subseteq \left\{ Z_{\stoppingTime} < \beta_{\stoppingTime}(\failureProb)\right\}.
\end{align}

Expanding on the definition of $\banditSpace_{alt}$ and $Z_t$, and noting that we're assuming arm 1 has the highest true mean:

\begin{align}
\left\{ 1 \not\in \maxMeanArm({\stoppingTime}) \right\} = \left\{ v \in \banditSpace_{alt}(\maxMeanArm({\stoppingTime})) \right\} \subseteq \left\{ \defineZedTee{\stoppingTime} < \beta_{\stoppingTime}(\failureProb)\right\}.
\end{align}

So our target equation is the following:

\label{tar:track-and-stop}
\begin{align}
\Prob( 1 \not\in \maxMeanArm({\stoppingTime})) \leq \Prob\left( \defineZedTee{\stoppingTime} \geq \beta_{\stoppingTime}(\failureProb)\right) \leq \failureProb.
\end{align}

\seperator

Let us define:

\begin{align*}
S_{i_s} := \frac{s}{2}(\vectorBanditMeans_{i}(\stoppingTime^{-1}(s)) - \armPopulationMean{i})^2 \textbf{ where } \stoppingTime^{-1}(s) := min \{ t \in \naturals : \stoppingTime_{i}(t) = s \}.
\end{align*}

By Lemma\ref{lem:upperBoundSeq} we have
\begin{align*}
\Prob(\exists s \in \naturals : S_{i_s} \geq \log{s(s+1)} + \log{\frac{1}{\failureProb}}) \leq \failureProb.
\end{align*}

If we let $g(m) = \log{m(m + 1)}$, we meet the conditions of Lemma\ref{lem:increasingSeqUpper}, so we get:
\begin{align*}
\Prob \left(\exists (s_i)_{i=1}^{k} : \sum_{i=1}^{k}S_{i_s} \geq k\log
\{{
(\sum_{i=1}s_i)(\sum_{i=1}s_i + 1)
}\}
+ x \right) \leq \left(\frac{x}{k}\right)^k e^{k-x}.
\end{align*}

Expanding out our definition of $S_{i_s}$ by summing over t, we get:

\begin{align*}
\Prob \left(\exists t \in \naturals : \frac{1}{2}\sum_{i=1}^{k}\totalFunction{i}{t}(\vectorBanditMeans_{i}(t) - \aDifBandit_{i}(t))^2 \geq k\log
\{{
t(t + 1)
}\}
+ x \right) \leq \left(\frac{x}{k}\right)^k e^{k-x}.
\end{align*}

We know that if $t \in \naturals$ exists, then the statement must also hold for $\stoppingTime \geq t$:
\begin{align}\label{eq:314RenameLater}
\Prob \left(\frac{1}{2}\sum_{i=1}^{k} \totalFunction{i}{\stoppingTime}(\vectorBanditMeans_{i}(\stoppingTime) - \aDifBandit_{i}(\stoppingTime))^2 \geq k\log
\{{
\stoppingTime(\stoppingTime + 1)
}\}
+ x \right) \leq \left(\frac{x}{k}\right)^k e^{k-x}.
\end{align}

Let's now define $\failureProbabilityFunction:[k,\infty) \rightarrow [1,\infty)$ by $\failureProbabilityFunction(x) := \left(\frac{x}{k}\right)^k e^{k-x}$ and observe that 
\begin{align*}
\failureProbabilityFunction'(x)=\frac{\left(\frac{x}{k}\right)^{k} \left(k - x\right) e^{k - x}}{x},
\end{align*}
which is strictly negative on $(k,\infty)$. Hence, $\failureProbabilityFunction$ is strictly decreasing with $\failureProbabilityFunction(k)=1$ and $\lim_{x \nearrow \infty} \failureProbabilityFunction(x)=0$. Thus, $\failureProbabilityFunction$ has a well-defined inverse $\failureProbabilityFunction^{-1}:[0,1] \rightarrow [0,k]$. Next, we define the quantity $\beta_{t}(\failureProb) := k \cdot \log\left\lbrace t(t+1)\right\rbrace + \failureProbabilityFunction^{-1}(\failureProb)$ for $\failureProb \in [0,1]$. Hence, by \eqref{eq:314RenameLater} we have
\begin{align}
 \Prob \left(\frac{1}{2}\sum_{i=1}^{k} \totalFunction{i}{\stoppingTime}\{\vectorBanditMeans_{i}(\stoppingTime) - \aDifBandit_{i}(\stoppingTime)\}^2 \geq \beta_{\stoppingTime}(\failureProb) \right) \leq \failureProb.
\end{align}
Thus, we have
\begin{align*}
\Prob \left(\frac{1}{2}\inf\limits_{\aDifBandit \in \banditSpace_{alt}(\vectorBanditMeans(\stoppingTime))}\sum_{i=1}^{k} \totalFunction{i}{\stoppingTime}(\vectorBanditMeans_{i}(\stoppingTime) - \aDifBandit_{i}(\stoppingTime))^2 \geq \beta_{\stoppingTime}(x) \right) \leq \failureProb(x). \qed
\end{align*}

We have now met the target equation \ref{tar:track-and-stop} equation, so 
\begin{align*}
\beta_{t}(x) = k * \log\left(t(t+1)\right) + x \\
Z_{t}(\vectorBanditMeans) = \frac{1}{2}\inf\limits_{\aDifBandit \in \banditSpace_{alt}(\vectorBanditMeans)}\left(\sum_{i=1}^{k} \totalFunction{i}{t}(\vectorBanditMeans_{i}(t) - \aDifBandit_{i})^2\right) \\
\epsilon = \failureProb(x) = \left(\frac{x}{k}\right)^k e^{k-x}.
\end{align*}
